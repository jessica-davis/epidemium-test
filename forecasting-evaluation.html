<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Epidemic Forecasting and Evaluation — Epidemium</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:wght@300;400;500;600;700&family=Just+Another+Hand&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="css/styles.css">
</head>
<body>
<!-- ============================================================
     HTML
     ============================================================ -->

    <div class="container">
        <a href="index.html" class="back-link">&larr; Epidemium</a>
        <h1>Epidemic Forecasting and Evaluation</h1>
        <p class="subtitle">Understanding forecast outputs, scoring rules, and model comparison</p>

        <div id="loading" class="loading">Loading data...</div>
        <div id="error" class="error" style="display:none;"></div>

        <div id="main-app" style="display:none;">

        <!-- Mobile hamburger -->
        <button class="mobile-menu-btn" id="mobile-menu-btn">&#9776; Contents</button>

        <div class="app-layout">
            <!-- Sidebar navigation -->
            <nav class="sidebar" id="sidebar">
              <div class="sidebar-inner">
                <div class="sidebar-header">
                    <span class="sidebar-title">Contents</span>
                </div>
                <ul class="nav-outline">
                    <li><a class="nav-item active" data-section="section-overview">Overview</a></li>
                    <li>
                        <span class="nav-heading">1. Forecast Outputs</span>
                        <ul>
                            <li><a class="nav-item" data-section="section-pi">a. Prediction Intervals</a></li>
                            <li><a class="nav-item" data-section="section-trends">b. Categorical Forecasts</a></li>
                            <li><a class="nav-item" data-section="section-binned">c. Binned Distributions</a></li>
                        </ul>
                    </li>
                    <li>
                        <span class="nav-heading">2. Evaluation Metrics</span>
                        <ul>
                            <li>
                                <span class="nav-subheading">a. Interval-based scoring</span>
                                <ul>
                                    <li><a class="nav-item" data-section="section-wis">i. Weighted Interval Score</a></li>
                                    <li><a class="nav-item" data-section="section-coverage">ii. Coverage</a></li>
                                </ul>
                            </li>
                            <li>
                                <span class="nav-subheading">b. Categorical scoring</span>
                                <ul>
                                    <li><a class="nav-item" data-section="section-brier">i. Brier Score</a></li>
                                    <li><span class="nav-item disabled">ii. Ranked Probability Score <span class="badge-soon">Soon</span></span></li>
                                </ul>
                            </li>
                            <li>
                                <span class="nav-subheading">c. Distribution-based scoring</span>
                                <ul>
                                    <li><a class="nav-item" data-section="section-log-score">i. Log Score</a></li>
                                </ul>
                            </li>
                        </ul>
                    </li>
                </ul>
              </div>
            </nav>

            <!-- Main content area -->
            <div class="main-content">
            <button class="sidebar-toggle" id="sidebar-toggle">
                <span class="arrow">&#9664;</span> <span class="label">Contents</span>
            </button>

            <!-- ==================== Overview ==================== -->
            <section id="section-overview" class="content-section active">
                <div class="overview-intro">
                    <p>Epidemic forecasts provide valuable situational awareness for public health decision-making. This was underscored during the COVID-19 pandemic, when decision-makers needed real-time information on the current and near-term trajectory of the outbreak. But epidemic forecasting predates COVID-19 — the CDC has run seasonal influenza forecasting challenges since [year], bringing together research teams to predict flu activity each season. While the specific targets and scoring methods have evolved over time, the core goal remains the same: generating accurate, timely predictions to inform public health response.</p>

                    <p>In this module, we define common types of epidemic forecast formats and introduce some methods used to evaluate them.</p>

                </div>

                <div class="overview-outline">
                    <div class="outline-card">
                        <h3>1. Epidemic Forecast Outputs</h3>
                        <p>What types of predictions do epidemic forecasters produce?</p>
                        <ul>
                            <li><a data-section="section-pi">Prediction Intervals</a> &mdash; Ranges where we expect future values to fall</li>
                            <li><a data-section="section-trends">Categorical Forecasts</a> &mdash; Trends, activity levels, and other categorical predictions</li>
                            <li><a data-section="section-binned">Binned Distributions</a> &mdash; Discretizing forecasts into probability bins</li>
                        </ul>
                    </div>
                    <div class="outline-card">
                        <h3>2. Evaluation Metrics</h3>
                        <p>How do we measure whether a forecast is any good?</p>
                        <ul>
                            <li><a data-section="section-wis">Weighted Interval Score</a> &mdash; Scoring prediction intervals</li>
                            <li><a data-section="section-coverage">Coverage</a> &mdash; Do observations land inside the intervals?</li>
                            <li><a data-section="section-brier">Brier Score</a> &mdash; Scoring categorical forecasts</li>
                            <li><a data-section="section-log-score">Log Score</a> &mdash; Scoring binned distribution forecasts</li>
                            <li><span class="coming-soon-text">Ranked Probability Score &mdash; Coming soon</span></li>
                        </ul>
                    </div>
                </div>

                <div class="section-paging">
                    <span></span>
                    <a class="page-btn page-btn-next" data-section="section-pi"><span class="page-label">Next</span><span class="page-title">Prediction Intervals &rarr;</span></a>
                </div>
            </section>

            <!-- ==================== Prediction Intervals ==================== -->
            <section id="section-pi" class="content-section">
                <div class="content-wrapper">
                    <div class="chart-section">
                        <div class="chart-container">
                            <h3 class="chart-title">Weekly Incident Flu Hospitalizations &mdash; US National</h3>
                            <div class="controls">
                                <div class="control-group">
                                    <label>Prediction Intervals:</label>
                                    <button class="toggle-btn active" data-interval="95">95%</button>
                                    <button class="toggle-btn active" data-interval="80">80%</button>
                                    <button class="toggle-btn active" data-interval="50">50%</button>
                                </div>
                            </div>
                            <div id="chart"></div>
                        </div>
                    </div>
                    <div class="description-section">
                        <h3>What are Prediction Intervals?</h3>
                        <p class="definition">
                            Prediction intervals quantify uncertainty in forecasts. They provide a range where we expect future observations to fall with a specified probability.
                        </p>
                        <div class="example">
                            <strong>Example: What does an 80% Prediction Interval mean?</strong>
                            <p>If we generate 100 forecasts with 80% prediction intervals, we expect approximately 80 of the observed values to fall within their respective intervals. The remaining 20 observations would fall outside.</p>
                            <p style="margin-top: 15px;">In other words, an 80% PI tells us: "We are 80% confident that the true future value will fall somewhere between the lower and upper bounds of this interval."</p>
                        </div>
                    </div>
                </div>

                <div class="section-paging">
                    <a class="page-btn page-btn-prev" data-section="section-overview"><span class="page-label">Previous</span><span class="page-title">&larr; Overview</span></a>
                    <a class="page-btn page-btn-next" data-section="section-trends"><span class="page-label">Next</span><span class="page-title">Categorical Forecasts &rarr;</span></a>
                </div>
            </section>

            <!-- ==================== Categorical Forecasts ==================== -->
            <section id="section-trends" class="content-section">
                <h2 class="section-heading" style="margin-bottom:5px;">Categorical Forecasts</h2>

                <p class="section-sub">Rather than predicting a specific value or interval, sometimes it's more useful to predict the probability of an <em>outcome</em>. We often refer to these as <em>categorical forecasts</em>. These forecasts are often just as useful for public health decision-making because they provide a straight-forward picture of uncertainty and risk levels. Common examples include disease activity levels (e.g., how does current activity compare to previous seasons?) (right) and trend direction (do we expect cases to increase, decrease, or stay the same over the coming weeks?) (left).</p>


                <div class="gauge-row" style="display:flex; gap:40px; justify-content:center; flex-wrap:wrap; margin:24px 0; position:relative;">
                    <div class="gauge-panel" style="text-align:center; flex:1; min-width:260px; max-width:400px;">
                        <h3 class="chart-title" style="margin-bottom:4px;">Rate Trend</h3>
                        <p style="font-size:0.82rem; color:#666; margin-bottom:8px;">Direction of change</p>
                        <div id="trend-gauge"></div>
                    </div>
                    <div class="gauge-panel" style="text-align:center; flex:1; min-width:260px; max-width:400px;">
                        <h3 class="chart-title" style="margin-bottom:4px;">Activity Level</h3>
                        <p style="font-size:0.82rem; color:#666; margin-bottom:8px;">Intensity of disease burden</p>
                        <div id="activity-gauge"></div>
                    </div>
                    <span class="gauge-info-wrap" style="position:absolute; bottom:0; right:0;"><button class="gauge-info-btn" aria-label="How is the dial calculated?">i</button><span class="gauge-info-tip" style="right:0; left:auto; transform:none;">The dial position is a probability-weighted average across all categories. Each category is assigned an equal arc on the gauge, and the needle points to &Sigma; p<sub>i</sub> &middot; &theta;<sub>i</sub>, where p<sub>i</sub> is the forecast probability and &theta;<sub>i</sub> is the center angle of each segment.</span></span>
                </div>

                <div style="display:flex; gap:30px; flex-wrap:wrap; margin:20px 0;">
                    <div style="flex:1; min-width:280px;">
                        <h3 class="chart-title">Rate Trend Forecast</h3>
                        <div id="trend-chart"></div>
                    </div>
                    <div style="flex:1; min-width:280px;">
                        <h3 class="chart-title">Activity Level Forecast</h3>
                        <div id="activity-chart"></div>
                    </div>
                </div>

                <hr class="section-divider">

                <div class="content-wrapper">
                    <div class="chart-section">
                        <h3 class="section-heading" style="font-size:1.2rem; margin-bottom:10px;">Why use categorical forecasts?</h3>
                        <p style="font-size:0.9rem; margin-bottom:12px;">During periods of rapid epidemic change, precise count predictions are often unreliable. A model might miss the exact number of hospitalizations by a wide margin, yet still correctly signal that activity is <strong>increasing</strong> or has reached a <strong>high</strong> level. Categorical forecasts capture this coarser but highly actionable information.</p>
                        <p style="font-size:0.9rem; margin-bottom:12px;">For public health decision-makers, knowing <em>whether</em> activity is rising matters more than knowing the exact count. Categorical outputs map directly to response thresholds &mdash; staffing decisions, public messaging, and resource allocation &mdash; without requiring stakeholders to interpret numeric intervals.</p>
                        <p style="font-size:0.9rem; margin-bottom:12px;">Categorical forecasts also remain informative when quantile forecasts break down, such as early in a season when data are sparse or when a novel pathogen makes historical baselines unreliable. Because they compress the prediction space into a few well-defined outcomes, they are more robust to model misspecification and easier to communicate to non-technical audiences.</p>
                    </div>
                    <div class="description-section">
                        <h3>Evaluating Categorical Forecasts</h3>
                        <p>How do we measure whether a categorical forecast is any good? We use <strong>proper scoring rules</strong> that reward both accuracy and well-calibrated probabilities:</p>
                        <ul style="list-style:none; padding:0; margin-top:12px;">
                            <li style="margin-bottom:10px;"><a data-section="section-brier" style="padding:6px 0; border:none; font-size:0.88rem; cursor:pointer;"><strong>Brier Score</strong> &mdash; scoring categorical forecasts</a></li>
                            <li><span class="nav-item disabled" style="padding:6px 0; border:none; font-size:0.88rem;"><strong>Ranked Probability Score</strong> &mdash; scoring ordered categories <span class="badge-soon">Coming soon</span></span></li>
                        </ul>
                    </div>
                </div>

                <div class="citation" style="margin-top:20px;">
                    Biggerstaff M, et al. (2024). Responding to the return of influenza in the United States with improved epidemic forecasting. <em>JMIR Public Health and Surveillance</em>, 10, e54340. <a href="https://doi.org/10.2196/54340" target="_blank" rel="noopener">doi:10.2196/54340</a>
                </div>
                <div class="citation" style="margin-top:8px;">
                    Mathis SM, et al. (2025). Framework for classifying disease trends applied to influenza-associated hospital admissions in the United States. <em>Open Forum Infectious Diseases</em>, 12(8), ofaf460. <a href="https://doi.org/10.1093/ofid/ofaf460" target="_blank" rel="noopener">doi:10.1093/ofid/ofaf460</a>
                </div>
                <div class="citation" style="margin-top:8px;">
                    Gneiting T &amp; Raftery AE (2007). Strictly proper scoring rules, prediction, and estimation. <em>Journal of the American Statistical Association</em>, 102(477), 359&ndash;378. <a href="https://doi.org/10.1198/016214506000001437" target="_blank" rel="noopener">doi:10.1198/016214506000001437</a>
                </div>

                <div class="section-paging">
                    <a class="page-btn page-btn-prev" data-section="section-pi"><span class="page-label">Previous</span><span class="page-title">&larr; Prediction Intervals</span></a>
                    <a class="page-btn page-btn-next" data-section="section-binned"><span class="page-label">Next</span><span class="page-title">Binned Distributions &rarr;</span></a>
                </div>
            </section>

            <!-- ==================== Binned Distributions ==================== -->
            <section id="section-binned" class="content-section">
                <h2 class="section-heading" style="margin-bottom:5px;">Binned Distributions</h2>
                <p class="section-sub">Discretizing a continuous forecast into probability bins &mdash; and why the bin width matters.</p>

                <div class="content-wrapper">
                    <div class="chart-section">
                        <div class="chart-container">
                            <h3 class="chart-title">Explore: Binning a Forecast Distribution</h3>
                            <p style="font-size:0.85rem; color:#666; margin-bottom:8px;">A forecast for weekly hospitalizations, N(500, 80). Adjust the bin width to see how it changes the histogram.</p>
                            <div class="boxplot-controls">
                                <div class="control-group">
                                    <label>Bin width:</label>
                                    <span class="width-label">Narrow</span>
                                    <input type="range" class="wis-slider" id="binned-width-slider" min="10" max="200" step="5" value="50">
                                    <span class="width-label">Wide</span>
                                    <span class="width-label" id="binned-width-value" style="font-weight:700;">50</span>
                                </div>
                            </div>
                            <div id="binned-explorer"></div>
                            <div id="binned-info-display" style="margin-top:8px; font-size:0.88rem; color:#555;"></div>
                        </div>
                    </div>
                    <div class="description-section">
                        <h3>What are Binned Distributions?</h3>
                        <p class="definition">
                            A binned distribution discretizes a continuous forecast into non-overlapping intervals (bins), each assigned a probability mass representing how likely the observed value is to fall in that range.
                        </p>
                        <p style="margin-top:10px;">For each bin [a, b), the probability is:</p>
                        <div class="math-eq" style="text-align:center; font-size:1.1rem; margin:8px 0;">
                            P(bin) = CDF(b) &minus; CDF(a)
                        </div>
                        <p>All bin probabilities sum to 1, forming a proper probability mass function (PMF).</p>
                        <p style="margin-top:12px;"><strong>Why bin?</strong> Binned distributions are a standard reporting format for epidemic forecasts. They convert a continuous predictive distribution into a finite set of categories that are easy to communicate, store, and score.</p>
                        <div class="example" style="margin-top:14px;">
                            <strong>Try it:</strong> Drag the bin width slider. Narrow bins give a fine-grained shape that closely traces the smooth curve. Wide bins produce a coarse, blocky histogram that loses detail.
                        </div>
                    </div>
                </div>

                <hr class="section-divider">

                <h2 class="section-heading" style="font-size:1.2rem; margin-bottom:5px;">Why Bin Width Matters</h2>
                <p class="section-sub">The same forecast at three different bin widths.</p>

                <div class="content-wrapper">
                    <div class="chart-section">
                        <div class="chart-container">
                            <div id="binned-comparison"></div>
                        </div>
                    </div>
                    <div class="description-section">
                        <h3>The Resolution Tradeoff</h3>
                        <p><strong>Narrow bins</strong> (left) give fine resolution &mdash; the histogram closely approximates the underlying probability density. But each bin captures only a small slice of probability.</p>
                        <p style="margin-top:10px;"><strong>Wide bins</strong> (right) are coarser &mdash; the shape is blocky and you lose detail about where exactly the peak is. But each bin captures more probability mass.</p>
                        <p style="margin-top:10px;">The &ldquo;right&rdquo; bin width depends on the use case. For scoring, it has a direct effect: the <a data-section="section-log-score" style="cursor:pointer; text-decoration:underline;">Log Score</a> illustrates this tradeoff directly.</p>
                    </div>
                </div>

                <div class="section-paging">
                    <a class="page-btn page-btn-prev" data-section="section-trends"><span class="page-label">Previous</span><span class="page-title">&larr; Categorical Forecasts</span></a>
                    <a class="page-btn page-btn-next" data-section="section-wis"><span class="page-label">Next</span><span class="page-title">Weighted Interval Score &rarr;</span></a>
                </div>
            </section>

            <!-- ==================== Weighted Interval Score ==================== -->
            <section id="section-wis" class="content-section">

                <!-- Section 1: Evaluating Quantile Forecasts -->
                <h2 class="section-heading" style="margin-bottom:5px;">Evaluating Quantile Forecasts</h2>
                <p class="section-sub">How do we measure whether a probabilistic forecast is any good?</p>

                <div class="content-wrapper" style="margin-bottom:10px;">
                    <div class="chart-section">
                        <div class="chart-container">
                            <h3 class="chart-title">Forecast as Prediction Intervals</h3>
                            <div class="controls">
                                <div class="control-group">
                                    <label>Show PIs:</label>
                                    <button class="toggle-btn active" data-wis-box-interval="95">95%</button>
                                    <button class="toggle-btn active" data-wis-box-interval="80">80%</button>
                                    <button class="toggle-btn active" data-wis-box-interval="50">50%</button>
                                </div>
                            </div>
                            <div id="wis-box-chart"></div>
                        </div>
                    </div>
                    <div class="description-section">
                        <h3>Sharpness vs. Calibration</h3>
                        <p>Quantile forecasts express uncertainty as prediction intervals. But how do we judge their quality? Two key properties:</p>
                        <p><strong>Sharpness</strong> &mdash; Are the intervals narrow? A forecast that says "between 0 and 100,000" is uninformative. Narrower intervals are more useful.</p>
                        <p><strong>Calibration</strong> &mdash; Do the observed values actually land inside the intervals at the right rate? An 80% PI should contain the observation about 80% of the time.</p>
                        <p class="definition" style="margin-top:15px;">The ideal forecast is as sharp as possible while remaining well-calibrated. We need a scoring rule that rewards both properties.</p>
                    </div>
                </div>

                <hr class="section-divider">

                <!-- Section 2: The WIS Score -->
                <h2 class="section-heading" style="margin-bottom:5px;">The Weighted Interval Score</h2>
                <p class="section-sub">Explore how interval width and observed values affect the score.</p>

                <div class="content-wrapper" style="margin-bottom:10px;">
                    <div class="chart-section">
                        <div class="boxplot-container">
                            <h3 class="chart-title">Explore: Single Forecast Date</h3>
                            <div class="boxplot-controls">
                                <div class="control-group">
                                    <label>Show PIs:</label>
                                    <button class="toggle-btn active" data-bp-interval="95">95%</button>
                                    <button class="toggle-btn active" data-bp-interval="80">80%</button>
                                    <button class="toggle-btn active" data-bp-interval="50">50%</button>
                                </div>
                                <div class="control-group">
                                    <label>Width:</label>
                                    <span class="width-label">Sharp</span>
                                    <input type="range" class="wis-slider" id="bp-width-slider" min="0.3" max="3.0" step="0.05" value="1.0">
                                    <span class="width-label">Wide</span>
                                    <span class="width-label" id="bp-width-value" style="font-weight:700;">1.0x</span>
                                </div>
                            </div>
                            <div id="boxplot"></div>
                            <p class="drag-hint">Drag the observed value (dark dot) left or right to see how the score changes.</p>
                            <div id="bp-coverage"></div>
                        </div>
                    </div>
                    <div class="description-section">
                        <h3>What is WIS?</h3>
                        <p class="definition">
                            WIS measures forecast quality by balancing <strong>sharpness</strong> (narrow intervals) with <strong>calibration</strong> (observations landing inside). Lower is better.
                        </p>
                        <p>WIS decomposes into three penalties:</p>
                        <div class="wis-component">
                            <span class="swatch" style="background:#5ACDC5;"></span>
                            <span><strong>Dispersion</strong> &mdash; how wide the intervals are</span>
                        </div>
                        <div class="wis-component">
                            <span class="swatch" style="background:#E87D5F;"></span>
                            <span><strong>Underprediction</strong> &mdash; observed &gt; upper bound</span>
                        </div>
                        <div class="wis-component">
                            <span class="swatch" style="background:#8B6DB0;"></span>
                            <span><strong>Overprediction</strong> &mdash; observed &lt; lower bound</span>
                        </div>

                        <div id="wis-decomp" style="margin-top:12px;"></div>

                        <div class="wis-scores">
                            <div class="wis-score-row">
                                <span class="wis-score-label">WIS</span>
                                <span class="wis-score-value" id="wis-model-val">&mdash;</span>
                            </div>
                        </div>
                        <p style="font-size:0.78rem; color:#888; margin-top:10px;">* The scale of WIS depends on the magnitude of the forecast (see <em>The Math Behind WIS: Scale dependence and relative WIS</em>).</p>
                    </div>
                </div>

                <!-- Section 3: Collapsible math details -->
                <details class="math-details">
                    <summary>The Math Behind WIS</summary>
                    <div class="math-body">
                        <p><strong>Connection to CRPS.</strong> The Weighted Interval Score is a discrete approximation of the <strong>Continuous Ranked Probability Score (CRPS)</strong>, a standard scoring rule for probabilistic forecasts:</p>

                        <div class="math-eq">
                            CRPS(F, y) = &int;<sub>&minus;&infin;</sub><sup>&infin;</sup> (F(x) &minus; 1[x &ge; y])&sup2; dx
                        </div>

                        <p>where F is the forecast CDF and y is the observed value. CRPS measures the integrated squared distance between the forecast distribution and the step function at the observation. A perfect forecast gives CRPS = 0.</p>

                        <p>As the number of quantiles increases, WIS converges to CRPS, making it a practical way to score quantile-based forecasts.</p>

                        <span class="math-label">Interval Score for a (1&minus;&alpha;) prediction interval:</span>
                        <div class="math-eq">
                            IS<sub>&alpha;</sub>(l, u, y) = (u &minus; l) + (2/&alpha;)(l &minus; y) &middot; 1[y &lt; l] + (2/&alpha;)(y &minus; u) &middot; 1[y &gt; u]
                        </div>

                        <p>The three terms correspond to the three WIS components:</p>
                        <p><strong style="color:#5ACDC5;">(u &minus; l)</strong> = <strong>Dispersion</strong> &mdash; penalizes wide intervals. Wider intervals always increase this term.</p>
                        <p><strong style="color:#E87D5F;">(2/&alpha;)(y &minus; u)</strong> = <strong>Underprediction</strong> &mdash; when the observed value y falls above the upper bound u. The 2/&alpha; scaling means narrower PIs receive harsher penalties for misses:</p>
                        <ul>
                            <li>Missing a 95% PI (&alpha; = 0.05): penalty multiplier = 2/0.05 = <strong>40&times;</strong></li>
                            <li>Missing an 80% PI (&alpha; = 0.20): penalty multiplier = 2/0.20 = <strong>10&times;</strong></li>
                            <li>Missing a 50% PI (&alpha; = 0.50): penalty multiplier = 2/0.50 = <strong>4&times;</strong></li>
                        </ul>
                        <p>This asymmetric weighting ensures that forecasters cannot simply widen all intervals to guarantee coverage &mdash; the dispersion penalty counterbalances overly wide intervals.</p>
                        <p><strong style="color:#8B6DB0;">(2/&alpha;)(l &minus; y)</strong> = <strong>Overprediction</strong> &mdash; when y falls below the lower bound l, same scaling.</p>

                        <span class="math-label">Weighted Interval Score:</span>
                        <div class="math-eq">
                            WIS = 1/(K + 0.5) &middot; [ 0.5 &middot; |y &minus; m| + &sum;<sub>k=1</sub><sup>K</sup> (&alpha;<sub>k</sub>/2) &middot; IS<sub>&alpha;k</sub> ]
                        </div>

                        <p>where <em>m</em> is the predicted median, <em>K</em> is the number of interval pairs, and the weights &alpha;<sub>k</sub>/2 ensure that WIS converges to CRPS as the number of quantiles increases. The 0.5 &middot; |y &minus; m| term is the median absolute error, giving additional weight to point accuracy.</p>

                        <p><strong>Propriety.</strong> WIS is a <em>proper scoring rule</em>, meaning it is minimized in expectation when the forecaster reports their true belief about the future. This property incentivizes honest forecasting &mdash; a forecaster cannot systematically improve their score by misrepresenting their uncertainty.</p>

                        <span class="math-label">Scale dependence and relative WIS:</span>
                        <p>WIS is measured in the same units as the forecast target (e.g., hospitalizations). If a location has 10&times; more hospitalizations, both the interval widths and any miss penalties scale by roughly 10&times;, producing a WIS that is roughly 10&times; larger &mdash; even if the forecast is equally skillful.</p>
                        <p>To normalize, we compute the <strong>relative WIS</strong> as a ratio against a baseline model, evaluated on the same targets.</p>

                        <div class="citation">
                            Bracher J, Ray EL, Gneiting T, Reich NG (2021). Evaluating epidemic forecasts in an interval format. <em>PLOS Computational Biology</em> 17(2): e1008618. <a href="https://doi.org/10.1371/journal.pcbi.1008618" target="_blank" rel="noopener">doi:10.1371/journal.pcbi.1008618</a>
                        </div>
                    </div>
                </details>

                <hr class="section-divider">

                <!-- Section 4: Relative WIS -->
                <h2 class="section-heading" style="margin-bottom:5px;">Relative WIS &mdash; Comparing to a Baseline</h2>
                <p class="section-sub">A score only has meaning in comparison. How does the model stack up against a simple baseline?</p>

                <p style="font-size:0.9rem; margin-bottom:12px;">Because WIS is computed in the same units as the forecast target, its magnitude scales directly with the size of the numbers being predicted. A state like California, with thousands of weekly hospitalizations, will naturally produce wider prediction intervals &mdash; and therefore larger WIS values &mdash; than a state like Vermont with only a handful of cases. Comparing raw WIS across locations, or even across time periods with different baseline activity, tells you more about the <em>scale of the epidemic</em> than the <em>quality of the forecast</em>.</p>
                <p style="font-size:0.9rem; margin-bottom:20px;">The solution is to divide each model's WIS by the WIS of a <strong>reference baseline</strong> model, producing a <strong>relative WIS (rWIS)</strong>. Because both the model and the baseline face the same forecasting difficulty at each location and time point, the ratio cancels out differences in scale &mdash; letting us ask: "Does this model add value beyond a simple, naive forecast?" regardless of whether we are forecasting 10 cases or 10,000.</p>

                <div class="content-wrapper">
                    <div class="chart-section">
                        <div class="chart-container">
                            <h3 class="chart-title">Model vs. Baseline Forecast</h3>
                            <div id="rwis-chart"></div>
                        </div>
                    </div>
                    <div class="description-section">
                        <h3>The FluSight Baseline</h3>
                        <p>The <strong>FluSight baseline</strong> is a simple persistence model: it predicts the most recent observed value forward, using historical first differences to generate a prediction distribution.</p>
                        <div class="math-eq" style="text-align:center; font-size:1.3rem; margin-top:12px;">
                            rWIS = WIS<sub>model</sub> / WIS<sub>baseline</sub>
                        </div>
                        <p><strong>rWIS &lt; 1</strong> means the model outperforms the baseline.<br>
                        <strong>rWIS &gt; 1</strong> means the baseline is better.</p>
                    </div>
                </div>

                <div class="section-paging">
                    <a class="page-btn page-btn-prev" data-section="section-binned"><span class="page-label">Previous</span><span class="page-title">&larr; Binned Distributions</span></a>
                    <a class="page-btn page-btn-next" data-section="section-coverage"><span class="page-label">Next</span><span class="page-title">Coverage &rarr;</span></a>
                </div>
            </section>

            <!-- ==================== Coverage ==================== -->
            <section id="section-coverage" class="content-section">
                <h2 class="section-heading" style="margin-bottom:5px;">Coverage</h2>
                <p class="section-sub">Does the observed value actually fall inside the prediction interval? Coverage measures how often it does.</p>

                <div class="content-wrapper">
                    <div class="chart-section">
                        <div class="chart-container">
                            <h3 class="chart-title">Forecast Intervals vs. Observed (Horizon 0) <span class="gauge-info-wrap"><button class="gauge-info-btn" aria-label="About this chart">i</button><span class="gauge-info-tip">If you make a one-week-ahead forecast every week during the season, you can evaluate your overall coverage by checking how often the observed value falls inside each prediction interval across all forecast dates.</span></span></h3>
                            <div class="controls">
                                <div class="control-group">
                                    <label>PI:</label>
                                    <button class="toggle-btn active" data-cov-interval="50">50%</button>
                                    <button class="toggle-btn" data-cov-interval="80">80%</button>
                                    <button class="toggle-btn" data-cov-interval="90">90%</button>
                                    <button class="toggle-btn" data-cov-interval="95">95%</button>
                                    <button class="toggle-btn" data-cov-interval="98">98%</button>
                                </div>
                                <div class="control-group">
                                    <button class="toggle-btn" id="calc-coverage-btn" style="background:#1E9CC5; color:white; border-color:#1E9CC5;">Calculate Coverage</button>
                                </div>
                            </div>
                            <div id="coverage-chart"></div>
                            <div id="coverage-results" style="display:none; margin-top:16px;"></div>
                        </div>
                    </div>
                    <div class="description-section">
                        <h3>What is Coverage?</h3>
                        <p>For a single forecast, coverage is binary: the observation is either <strong>inside</strong> the prediction interval (coverage = 1) or <strong>outside</strong> (coverage = 0). There is no partial credit.</p>
                        <p>To evaluate a model, we aggregate across many forecasts. If we compute 27 forecasts with an 80% PI, we count how many times the observation lands inside. If 22 out of 27 observations are covered, the <strong>empirical coverage</strong> is 22/27 &asymp; 81%.</p>
                        <p class="definition" style="margin-top:12px;">A well-calibrated X% prediction interval should contain the observed value approximately X% of the time.</p>
                        <p>Select a PI level and click <strong>Calculate Coverage</strong> to see which forecasts are covered.</p>
                    </div>
                </div>

                <hr class="section-divider">

                <h2 class="section-heading" style="font-size:1.2rem; margin-bottom:5px;">Coverage Across All Prediction Intervals</h2>
                <p class="section-sub">How does empirical coverage compare to the nominal level across all PI widths?</p>

                <div class="content-wrapper">
                    <div class="chart-section">
                        <div class="chart-container">
                            <h3 class="chart-title">Coverage Curve</h3>
                            <div id="coverage-curve-chart"></div>
                        </div>
                    </div>
                    <div class="description-section">
                        <h3>Reading the Coverage Curve</h3>
                        <p>The <strong>dashed diagonal</strong> represents perfect calibration: a 50% PI covers 50% of observations, a 90% PI covers 90%, and so on.</p>
                        <p><strong>Above the diagonal</strong> &mdash; the model is <strong>underconfident</strong>. Its intervals are wider than necessary, capturing more observations than expected.</p>
                        <p><strong>Below the diagonal</strong> &mdash; the model is <strong>overconfident</strong>. Its intervals are too narrow, missing observations more often than they should.</p>
                        <p class="definition" style="margin-top:12px;">The closer the curve tracks the diagonal, the better calibrated the forecast is.</p>
                    </div>
                </div>

                <div class="section-paging">
                    <a class="page-btn page-btn-prev" data-section="section-wis"><span class="page-label">Previous</span><span class="page-title">&larr; Weighted Interval Score</span></a>
                    <a class="page-btn page-btn-next" data-section="section-brier"><span class="page-label">Next</span><span class="page-title">Brier Score &rarr;</span></a>
                </div>
            </section>

            <!-- ==================== Brier Score ==================== -->
            <section id="section-brier" class="content-section">
                <h2 class="section-heading" style="margin-bottom:5px;">Brier Score</h2>
                <p class="section-sub">How do we score a categorical probability forecast against the observed outcome?</p>

                <div class="content-wrapper">
                    <div class="chart-section">
                        <div class="chart-container">
                            <h3 class="chart-title">Explore: Score a Single Forecast</h3>
                            <p style="font-size:0.85rem; color:#666; margin-bottom:8px;">This is the same rate trend forecast from the Categorical Forecasts section. Select an observed outcome to see how it is scored.</p>
                            <div class="controls">
                                <div class="control-group" style="flex-wrap:wrap; gap:6px;">
                                    <label>Observed:</label>
                                    <button class="toggle-btn" data-brier-outcome="Large Increase">Large Increase</button>
                                    <button class="toggle-btn active" data-brier-outcome="Increase">Increase</button>
                                    <button class="toggle-btn" data-brier-outcome="Stable">Stable</button>
                                    <button class="toggle-btn" data-brier-outcome="Decrease">Decrease</button>
                                    <button class="toggle-btn" data-brier-outcome="Large Decrease">Large Decrease</button>
                                </div>
                            </div>
                            <div id="brier-explorer"></div>
                            <div id="brier-calc-display"></div>
                        </div>
                    </div>
                    <div class="description-section">
                        <h3>What is the Brier Score?</h3>
                        <p class="definition">
                            The Brier Score measures the accuracy of a categorical probability forecast. It sums the squared differences between the forecast probabilities and the outcome indicator for each category:
                        </p>
                        <div class="math-eq" style="text-align:center; font-size:1.3rem; margin:12px 0;">
                            BS = &sum;<sub>i=1</sub><sup>K</sup> (p<sub>i</sub> &minus; o<sub>i</sub>)&sup2;
                        </div>
                        <p>where <strong>p<sub>i</sub></strong> is the forecast probability for category <em>i</em>, and <strong>o<sub>i</sub></strong> = 1 for the observed category and 0 for all others.</p>
                        <ul style="margin-top:10px; padding-left:18px; font-size:0.9rem;">
                            <li><strong>BS = 0</strong> &mdash; perfect forecast (all probability on the correct category)</li>
                            <li><strong>BS = 2</strong> &mdash; worst possible (all probability on one wrong category)</li>
                        </ul>
                        <p style="margin-top:12px;">The Brier Score is a <strong>proper scoring rule</strong> &mdash; a forecaster minimizes their expected score by reporting their true beliefs. The biggest penalty comes from not putting enough probability on the category that actually occurs.</p>
                        <div class="example" style="margin-top:14px;">
                            <strong>Try it:</strong> Select different observed outcomes on the left to see how the Brier Score changes. Notice that the score is lowest when the forecast placed the most probability on the correct category.
                        </div>
                    </div>
                </div>

                <hr class="section-divider">

                <!-- Season-Long Evaluation -->
                <h2 class="section-heading" style="font-size:1.2rem; margin-bottom:5px;">Season-Long Evaluation</h2>
                <p class="section-sub">Averaging the Brier Score across many forecasts to evaluate overall performance.</p>

                <div class="content-wrapper">
                    <div class="chart-section">
                        <div class="chart-container">
                            <h3 class="chart-title">Rate Trend Forecast &mdash; 12 Week Season</h3>
                            <div id="brier-season-chart"></div>
                            <div id="brier-season-results"></div>
                        </div>
                    </div>
                    <div class="description-section">
                        <h3>Aggregating Brier Scores</h3>
                        <p>For a single forecast, the Brier Score tells us how far the predicted probabilities were from the true outcome. But one forecast alone doesn&rsquo;t tell us much &mdash; we need to evaluate performance over many forecasts.</p>
                        <p style="margin-top:10px;">The <strong>average Brier Score</strong> across a season summarizes overall model quality. Lower is better.</p>
                        <p style="margin-top:10px;">We compare against a <strong>uniform baseline</strong> that always assigns equal probability (20%) to each of the five categories. For a uniform forecast, BS&nbsp;=&nbsp;0.80 regardless of the outcome. If the model&rsquo;s average BS is lower, it adds value beyond na&iuml;ve guessing.</p>
                        <div class="example" style="margin-top:14px;">
                            <strong>Reading the chart:</strong> The top panel shows each week&rsquo;s forecast as a stacked bar &mdash; taller segments indicate higher predicted probability. White dots mark the observed outcome. The bottom panel shows the resulting Brier Score: <span style="color:#79CAC4; font-weight:600;">teal</span>&nbsp;=&nbsp;BS&nbsp;&lt;&nbsp;0.50 (good), <span style="color:#E8A56D; font-weight:600;">tan</span>&nbsp;=&nbsp;BS&nbsp;&ge;&nbsp;0.50 (poor). The dashed line is the uniform baseline (BS&nbsp;=&nbsp;0.80).
                        </div>
                    </div>
                </div>

                <!-- Collapsible math details -->
                <details class="math-details">
                    <summary>The Math Behind the Brier Score</summary>
                    <div class="math-body">
                        <span class="math-label">Multi-category Brier Score:</span>
                        <div class="math-eq">
                            BS = &sum;<sub>i=1</sub><sup>K</sup> (p<sub>i</sub> &minus; o<sub>i</sub>)&sup2;
                        </div>
                        <p>where K is the number of categories, p<sub>i</sub> &isin; [0, 1] is the forecast probability for category i (with &sum;p<sub>i</sub> = 1), and o<sub>i</sub> &isin; {0, 1} is the outcome indicator (1 for the observed category, 0 otherwise). The score ranges from 0 (perfect) to 2 (worst possible).</p>

                        <p style="margin-top:12px;"><strong>Uniform baseline.</strong> A forecast that assigns equal probability 1/K to every category always produces BS = (1/K &minus; 1)&sup2; + (K&minus;1)(1/K)&sup2;. For K = 5 categories, this gives BS = 0.80 regardless of which category is observed.</p>

                        <span class="math-label">Average Brier Score:</span>
                        <div class="math-eq">
                            BS&#x0305; = (1/N) &sum;<sub>t=1</sub><sup>N</sup> BS<sub>t</sub>
                        </div>
                        <p>The average across N forecasts summarizes overall calibration and accuracy.</p>

                        <p style="margin-top:12px;"><strong>Connection to RPS.</strong> For <em>ordered</em> categorical outcomes, the <strong>Ranked Probability Score (RPS)</strong> extends the Brier Score by summing squared differences of <em>cumulative</em> probabilities. RPS respects the ordering of categories &mdash; predicting "Stable" when "Increase" occurs is penalized less than predicting "Large Decrease."</p>

                        <p style="margin-top:12px;"><strong>Propriety.</strong> The Brier Score is a <em>strictly proper scoring rule</em>: a forecaster&rsquo;s expected score is uniquely minimized when they report their true subjective probabilities. Inflating or hedging probabilities systematically worsens the expected score.</p>

                        <div class="citation" style="margin-top:16px;">
                            Gneiting T &amp; Raftery AE (2007). Strictly proper scoring rules, prediction, and estimation. <em>Journal of the American Statistical Association</em>, 102(477), 359&ndash;378. <a href="https://doi.org/10.1198/016214506000001437" target="_blank" rel="noopener">doi:10.1198/016214506000001437</a>
                        </div>
                    </div>
                </details>

                <div class="section-paging">
                    <a class="page-btn page-btn-prev" data-section="section-coverage"><span class="page-label">Previous</span><span class="page-title">&larr; Coverage</span></a>
                    <a class="page-btn page-btn-next" data-section="section-log-score"><span class="page-label">Next</span><span class="page-title">Log Score &rarr;</span></a>
                </div>
            </section>

            <!-- ==================== Log Score ==================== -->
            <section id="section-log-score" class="content-section">
                <h2 class="section-heading" style="margin-bottom:5px;">Log Score</h2>
                <p class="section-sub">How do we score a binned probability forecast? The log score rewards putting probability mass where the observation lands.</p>

                <div class="content-wrapper">
                    <div class="chart-section">
                        <div class="chart-container">
                            <h3 class="chart-title">Explore: Score a Binned Forecast</h3>
                            <p style="font-size:0.85rem; color:#666; margin-bottom:8px;">The same N(500, 80) forecast, binned and scored. Drag the observed value or change the bin width.</p>
                            <div class="boxplot-controls">
                                <div class="control-group">
                                    <label>Bin width:</label>
                                    <span class="width-label">Narrow</span>
                                    <input type="range" class="wis-slider" id="log-score-bw-slider" min="10" max="200" step="5" value="50">
                                    <span class="width-label">Wide</span>
                                    <span class="width-label" id="log-score-bw-value" style="font-weight:700;">50</span>
                                </div>
                            </div>
                            <div id="log-score-explorer"></div>
                            <p class="drag-hint">Drag the observed value (dark dot) left or right to see how the score changes.</p>
                            <div id="log-score-calc-display"></div>
                        </div>
                    </div>
                    <div class="description-section">
                        <h3>What is the Log Score?</h3>
                        <p class="definition">
                            The log score evaluates a binned forecast by measuring how much probability mass was assigned to the bin containing the observed value:
                        </p>
                        <div class="math-eq" style="text-align:center; font-size:1.3rem; margin:12px 0;">
                            LS = &minus;log<sub>2</sub>(p<sub>k</sub>)
                        </div>
                        <p>where <strong>p<sub>k</sub></strong> is the probability assigned to the bin that contains the observation.</p>
                        <ul style="margin-top:10px; padding-left:18px; font-size:0.9rem;">
                            <li><strong>LS = 0</strong> &mdash; perfect (all probability mass in the correct bin)</li>
                            <li><strong>LS &rarr; &infin;</strong> &mdash; worst (nearly zero mass on the correct bin)</li>
                        </ul>
                        <p style="margin-top:12px;">The log score is a <strong>proper scoring rule</strong> &mdash; a forecaster minimizes their expected score by reporting their true beliefs. It heavily penalizes assigning very low probability to the outcome that actually occurs.</p>
                        <div class="example" style="margin-top:14px;">
                            <strong>Try it:</strong> Drag the observed value into the tails &mdash; the score increases dramatically. Change the bin width to see how it affects the probability and therefore the score.
                        </div>
                    </div>
                </div>

                <hr class="section-divider">

                <!-- Bin Width Effect -->
                <h2 class="section-heading" style="font-size:1.2rem; margin-bottom:5px;">Bin Width Effect on Log Score</h2>
                <p class="section-sub">How does the choice of bin width change the score for the same forecast and observation?</p>

                <div class="content-wrapper">
                    <div class="chart-section">
                        <div class="chart-container">
                            <h3 class="chart-title">Log Score vs. Bin Width</h3>
                            <div id="log-score-bw-effect"></div>
                        </div>
                    </div>
                    <div class="description-section">
                        <h3>The Resolution&ndash;Score Tradeoff</h3>
                        <p><strong>Wider bins</strong> capture more probability mass &rarr; p<sub>k</sub> is larger &rarr; the log score is lower (better). But the forecast is less informative.</p>
                        <p style="margin-top:10px;"><strong>Narrower bins</strong> give each bin less mass &rarr; p<sub>k</sub> is smaller &rarr; the log score is higher (worse), even for a good forecast.</p>
                        <p class="definition" style="margin-top:12px;">This means log scores computed with different bin widths are <strong>not directly comparable</strong>. When comparing models, always use the same binning scheme.</p>
                        <p style="margin-top:10px;">As bin width &rarr; 0, the log score approaches &minus;log&thinsp;f(y)&thinsp;+&thinsp;log(&Delta;), where f(y) is the continuous density and &Delta; is the bin width. The constant log(&Delta;) offset cancels when comparing models.</p>
                    </div>
                </div>

                <hr class="section-divider">

                <!-- Season-Long Evaluation -->
                <h2 class="section-heading" style="font-size:1.2rem; margin-bottom:5px;">Season-Long Evaluation</h2>
                <p class="section-sub">Averaging the log score across a season of forecasts.</p>

                <div class="content-wrapper">
                    <div class="chart-section">
                        <div class="chart-container">
                            <h3 class="chart-title">Weekly Log Scores &mdash; 12 Week Season</h3>
                            <div id="log-score-season-chart"></div>
                            <div id="log-score-season-results"></div>
                        </div>
                    </div>
                    <div class="description-section">
                        <h3>Aggregating Log Scores</h3>
                        <p>The <strong>average log score</strong> across a season summarizes overall forecast quality. Lower is better.</p>
                        <p style="margin-top:10px;">We compare against a <strong>uniform baseline</strong> that assigns equal probability to every bin. For a uniform forecast, LS = log<sub>2</sub>(N) where N is the number of bins. If the model&rsquo;s average LS is lower, it adds value beyond na&iuml;ve guessing.</p>
                        <p style="margin-top:10px;">Each week&rsquo;s forecast is a normal distribution with different mean and spread, reflecting a realistic scenario where forecast uncertainty changes as the season progresses. The bin width slider above also updates these scores.</p>
                        <div class="example" style="margin-top:14px;">
                            <strong>Reading the chart:</strong> <span style="color:#79CAC4; font-weight:600;">Teal</span> bars indicate weeks where the model scored well (below 60% of baseline). <span style="color:#E8A56D; font-weight:600;">Tan</span> bars indicate weeks with poorer scores. The dashed line marks the uniform baseline.
                        </div>
                    </div>
                </div>

                <!-- Collapsible math details -->
                <details class="math-details">
                    <summary>The Math Behind the Log Score</summary>
                    <div class="math-body">
                        <span class="math-label">Log Score for a binned forecast:</span>
                        <div class="math-eq">
                            LS = &minus;log<sub>2</sub>(p<sub>k</sub>)
                        </div>
                        <p>where p<sub>k</sub> is the probability mass assigned to the bin [a<sub>k</sub>, b<sub>k</sub>) that contains the observed value y. The bin probability is computed from the forecast CDF: p<sub>k</sub> = F(b<sub>k</sub>) &minus; F(a<sub>k</sub>).</p>

                        <p style="margin-top:12px;"><strong>Continuous limit.</strong> As bin width &Delta; &rarr; 0, p<sub>k</sub> &asymp; f(y) &middot; &Delta;, so LS &asymp; &minus;log<sub>2</sub> f(y) &minus; log<sub>2</sub>(&Delta;). The &minus;log<sub>2</sub>(&Delta;) term is constant across models using the same bins, so model comparisons remain valid. The &minus;log<sub>2</sub> f(y) term is the continuous log score (ignorance score).</p>

                        <span class="math-label">Average Log Score:</span>
                        <div class="math-eq">
                            LS&#x0305; = (1/N) &sum;<sub>t=1</sub><sup>N</sup> LS<sub>t</sub>
                        </div>

                        <p style="margin-top:12px;"><strong>Propriety.</strong> The log score is a <em>strictly proper scoring rule</em>: a forecaster&rsquo;s expected score is uniquely minimized when reporting their true predictive distribution. It is the only proper scoring rule that is <em>local</em> &mdash; it depends only on the probability assigned to the event that actually occurred.</p>

                        <div class="citation" style="margin-top:16px;">
                            Gneiting T &amp; Raftery AE (2007). Strictly proper scoring rules, prediction, and estimation. <em>Journal of the American Statistical Association</em>, 102(477), 359&ndash;378. <a href="https://doi.org/10.1198/016214506000001437" target="_blank" rel="noopener">doi:10.1198/016214506000001437</a>
                        </div>
                    </div>
                </details>

                <div class="section-paging">
                    <a class="page-btn page-btn-prev" data-section="section-brier"><span class="page-label">Previous</span><span class="page-title">&larr; Brier Score</span></a>
                    <span></span>
                </div>
            </section>

            </div><!-- .main-content -->
        </div><!-- .app-layout -->
        </div><!-- #main-app -->
    </div><!-- .container -->

    <!-- External libraries -->
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <script src="https://unpkg.com/roughjs@4.6.6/bundled/rough.js"></script>

    <!-- App modules (load order matters) -->
    <script src="js/constants.js"></script>
    <script src="js/scoring.js"></script>
    <script src="js/pi-chart.js"></script>
    <script src="js/wis-charts.js"></script>
    <script src="js/categorical-charts.js"></script>
    <script src="js/coverage-charts.js"></script>
    <script src="js/brier-score.js"></script>
    <script src="js/binned-distribution.js"></script>
    <script src="js/log-score.js"></script>
    <script src="js/navigation.js"></script>
    <script src="js/app.js"></script>
</body>
</html>
